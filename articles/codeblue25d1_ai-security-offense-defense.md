---
title: "[レポート] 未知の技術に挑み、攻めて守るセキュリティ ～AIへの取り組みを通じて～ CODE BLUE 2025"
emoji: "🌐"
type: "idea"
topics: ["codeblue","security"]
published: false
publication_name: cscloud_blog
---

こんにちは、CSC の [CloudFastener](https://cloud-fastener.com/) というプロダクトで TAM のポジションで働いている平木です！

今回は、日本発の世界トップクラスの専門家による最先端の技術研究が発表される国際的なサイバーセキュリティカンファレンス、CODE BLUEに参加してきたため参加レポートを執筆します。

https://codeblue.jp/

## 「未知の技術に挑み、攻めて守るセキュリティ ～ AIへの取り組みを通じて ～ 」の概要

- Speakers: NRIセキュアテクノロジーズ株式会社 柿本 健司氏
- Category: OpenTalks
- Location: Track 2(HALL A)

NRIセキュアの、従来の枠にとらわれずブロックチェーンや宇宙分野などに挑む中から、特にAIへの取り組みを紹介します。  
講演では、画像分類モデルなどへの敵対的攻撃や、AIの検知をすり抜ける「敵対的Tシャツ」といった従来型AIの研究、および生成AIにおけるプロンプトインジェクションやAIレッドチーミングの取り組みを解説します。  
未知の技術に挑み、「攻めて守る」実践から新しい価値を生み出す同社の姿勢を伝えます。

https://codeblue.jp/program/time-table/day1-t2-opentalks-02/

## セッションレポート

NRIセキュアさんでは、先端技術への診断サービスの早期の取り組みを行っています。  
様々なサービス提供の根幹として、「新しい技術の普及に伴って新しい技術に対する新しいリスクがある」という考えから日々検証・サービス化を行っており、その取り組みについて今回のセッションではお話されていました。

その中で今回のセッションでは、

- 画像分類モデル、物体検知モデル
- 生成AI
- AIエージェント

をピックアップされています。

### 画像分類モデル、物体検知モデルの取り組み

まずは画像分類モデル、物体検知モデルにおける取り組みです。

画像分類モデルは、人の目に代わりAIモデルが画像を認識する技術であり、顔認証や自動運転などに利用されています。
物体検知モデルは、画像の中に「どこに・何が」あるかを検出し位置の特定やラベル付けを行うことから、監視カメラや製造ラインにおける不良品検査などに利用されています。

顔認証や監視カメラに対して不正がある場合に正常に動作しないリスクが考えられることから、攻撃事例を調査を行っています。

![](../images/codeblue25d1_ai-security-offense-defense_2025-11-18-19-44-18.png)
*柿本氏の講演スライドより*

画像分類モデルにおける有名な攻撃例として、微細な摂動（敵対的サンプル）が挙げられ、このような攻撃を受けることで誤分類を引き起こす可能性があります。

例えばパンダをパンダとして分類できなくなったり、自動運転の事例ではスピード表記を誤って認識してしまうことも懸念されます。

![](../images/codeblue25d1_ai-security-offense-defense_2025-11-18-19-45-51.png)
*柿本氏の講演スライドより*

物体検知モデルにおいては、特殊な敵対的パッチを活用することで人が正常に検出できず監視カメラの回避などへ応用されてしまう脅威となりえます。

![](../images/codeblue25d1_ai-security-offense-defense_2025-11-18-19-46-48.png)
*柿本氏の講演スライドより*

NRIセキュアさんでは、実際に攻撃事例に繋がりそうなものにおいては論文を読むだけでなく、実物での検証も実施しているようでパッチによって物の検出を誤認させたりする現象も確認できたとのことでした。

![](../images/codeblue25d1_ai-security-offense-defense_2025-11-18-19-47-11.png)
*柿本氏の講演スライドより*

実際に顧客のリスク評価を行った事例も紹介されています。

顧客事例では、物体検知モデルにおけるリスク評価を行うため、  
各条件を設け、各条件における攻撃シナリオを作成しペネトレーションテストを実施し、結果いずれも攻撃は成功した。

![](../images/codeblue25d1_ai-security-offense-defense_2025-11-18-19-47-59.png)
*柿本氏の講演スライドより*

ただし、攻撃に成功はしたものの攻撃を成功させるには様々なハードルがあることも確認され、環境に大きく左右されため、システム側に条件を加えることでリスクは低減できるようです。

例えば、撮影条件によっては光の反射により敵対パッチが作用しなかったり、素材などによっても

![](../images/codeblue25d1_ai-security-offense-defense_2025-11-18-19-50-40.png)
*柿本氏の講演スライドより*

### 生成AIにおける取り組み

続いて生成AIにおける取り組みの紹介です。

ChatGPTなどの大規模言語モデルは、SNSなどの他のサービスと比べても急激な普及速度があったことから、多くの利用が見込まれれば、攻撃事例もあると考え調査を始めたとのことです。

生成AIにおける一般的なリスクとしては、プロンプトインジェクションがあげられます。
攻撃者がシステムプロンプトを無視するようユーザープロンプトを投げることで意図しない応答を変えさせる攻撃をプロンプトインジェクションと呼ばれています。

![](../images/codeblue25d1_ai-security-offense-defense_2025-11-18-20-00-18.png)
*柿本氏の講演スライドより*

LLMを利用したアプリの診断の事例では他のリスクの気付きを得られたとのことで、それは周辺システムへの攻撃のリスクについてです。

攻撃者がLLMに対して汚染されたクエリをOpenSearchにリクエストする攻撃のリスクが見られた。

![](../images/codeblue25d1_ai-security-offense-defense_2025-11-18-20-05-41.png)
*柿本氏の講演スライドより*

周辺システムへの攻撃事例として、llama_indexにおける任意のコード実行が取り上げられました。

データへのクエリを自然言語でllama_indexが受け取るとデータフレームが解釈するクエリを生成します。
それを悪用し、悪意のあるコードを実行するプロンプトをリクエストすることで任意のOSコマンドを実行し、周辺システムへ悪影響を与えるケースが考えられます。

![](../images/codeblue25d1_ai-security-offense-defense_2025-11-18-20-10-30.png)
*柿本氏の講演スライドより*

![](../images/codeblue25d1_ai-security-offense-defense_2025-11-18-20-10-40.png)
*柿本氏の講演スライドより*

セッションでは、任意のコードを実行できるデモを実践いただき、OSコマンドとしてcatやremoveのコマンドを実行できる様子が確認できました。

単純なチャットボットでも設計によっては周辺システムへ悪影響与えてしまうということが理解できます。

NRIセキュアさんのAI Red Teamでは、周辺システムへの影響も含めた網羅的な評価が可能とのことです。

![](../images/codeblue25d1_ai-security-offense-defense_2025-11-18-20-14-00.png)
*柿本氏の講演スライドより*

### AIエージェントにおける取り組み

与えられた目標に沿って自律的に複雑なタスクを完遂可能なシステムであるAIエージェントは、
ユーザーから見たら中身はブラックボックスになってしまいます。

![](../images/codeblue25d1_ai-security-offense-defense_2025-11-18-21-39-17.png)
*柿本氏の講演スライドより*

セッションではシャドーリークという最近確認された脆弱性を紹介されました。

ShadowLeak（シャドーリーク）とは、事前に悪意のあるメールを仕込み、メールの要約をトリガーとして悪意のある指示に従って攻撃者サーバへ情報を送信してしまう攻撃がある。

https://www.radware.com/blog/threat-intelligence/shadowleak/

ユーザーはツールが何しているか分からないため、どんな被害を受けているのかを把握することが困難です。

![](../images/codeblue25d1_ai-security-offense-defense_2025-11-18-21-40-02.png)
*柿本氏の講演スライドより*

内部プロセスが見えづらいため、AIエージェントには正当な権限を付与することが重要になります。

![](../images/codeblue25d1_ai-security-offense-defense_2025-11-18-21-42-35.png)
*柿本氏の講演スライドより*

NRIセキュアさんでは、AI Red Team,AI Blue Team,AI Yellow Teamを掛け合わせ設計の段階で正当な評価を行うことでDevOpsのサイクルにおける包括的な評価を実施しているとのことです。

![](../images/codeblue25d1_ai-security-offense-defense_2025-11-18-21-42-53.png)
*柿本氏の講演スライドより*

最後の振り返りとして、取り組みの大きな流れを紹介いただきました。

1. 攻撃成功時にリスクが大きそうな対象を検討
2. 対象の攻撃事例の調査
3. 攻撃事例の検証・測定
4. サービス化

このサイクルを新しい技術で検証し続けていくことで様々な技術における評価を行うサービスを多く提供しているとのことでした。

![](../images/codeblue25d1_ai-security-offense-defense_2025-11-18-21-44-44.png)
*柿本氏の講演スライドより*

![](../images/codeblue25d1_ai-security-offense-defense_2025-11-18-21-44-53.png)
*柿本氏の講演スライドより*

## まとめ

こちらのセッションでは、
NRIセキュアさんが新しいい技術においてどのようにリスクを評価し、サービス化していったかの事例
について詳しく知ることができました。

この記事がどなたかの役に立つと嬉しいです。