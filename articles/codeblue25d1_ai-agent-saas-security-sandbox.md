---
title: "[レポート] AIエージェントSaaSを安全に提供するための自社サンドボックス基盤 CODE BLUE 2025"
emoji: "🌐"
type: "idea"
topics: ["codeblue","security"]
published: false
publication_name: cscloud_blog
---

こんにちは、CSC の [CloudFastener](https://cloud-fastener.com/) というプロダクトで TAM のポジションで働いている平木です！

今回は、日本発の世界トップクラスの専門家による最先端の技術研究が発表される国際的なサイバーセキュリティカンファレンス、CODE BLUEに参加してきたため参加レポートを執筆します。

https://codeblue.jp/

## 「AIエージェントSaaSを安全に提供するための自社サンドボックス基盤」の概要

- Speakers: GMO Flatt Security株式会社 pizzacat83氏
- Category: OpenTalks
- Location: Track 2(HALL A)

AIエージェントが自律的にファイルの読み書きやコード実行を行う際、意図しない振る舞いによるセキュリティリスクをコントロールするため、サンドボックス化が有効です。
本講演では、マルチテナントのAIエージェントSaaS「Takumi byGMO」を安全に提供するため、軽量なVMを高速にプロビジョンできる独自のサンドボックス基盤の開発と採用技術（Firecracker）について紹介します。
実践的な知見として、実際に社会実装されているAIエージェントのセキュリティを支える実行環境やエージェント設計に焦点を当てます。

https://codeblue.jp/program/time-table/day1-t2-opentalks-04/

## セッションレポート

AIエージェントを最大限活用する鍵は自律性と隔離ということを本セッションでは理解いただくための内容が解説されています。

GMO Flatt Securityのような、AIエージェントを提供するプロバイダーとしてサンドボックス環境を如何に早くセキュアに構築したかを説いています。

![](../images/codeblue25d1_ai-agent-saas-security-sandbox_2025-11-18-21-49-55.png)
*pizzacat83氏の講演スライドより*

AIエージェントは様々な操作を人間の介入なしに実行でき便利な反面、やってはいけないことをやってしまうリスクがあります。

例えば、重要なリポジトリのソースコードを削除するような破壊的な行動をとってしまったり、機密情報を公開する形で実装してしまったりなどです。

その対策としてよく見るもので、人間に「これやってもいいか？」という承認を求める仕様を見かけることが特にコーディングエージェントなどではあるが

- AIの自律性が損なわれてしまう
- 何回も承認操作することにより承認自体の形骸化のリスク

が発生してしまうことが考えられます。

![](../images/codeblue25d1_ai-agent-saas-security-sandbox_2025-11-18-22-30-22.png)
*pizzacat83氏の講演スライドより*

AIエージェントを最大限使いつつ、セキュリティを犠牲にしないためには適切な権限を割り当てたり、環境へのアクセスを制限をすることで損害の範囲を許容範囲内にできます。

| エージェントが扱う情報 | 外部アクセス権限 | 損害の許容範囲かどうか |
| --- | --- | --- |
| 機密文書 | インターネットアクセスなし | 許容できる |
| 機密文書 | インターネットへのフルアクセス | **許容できない** |
| 公開ウェブページ | インターネットへのフルアクセス | 許容できる |

インターネットへのアクセスがない中で自由に動かすように制限すれば、  
人間の承認なしでセキュリティを犠牲にしないことを実現できる。

![](../images/codeblue25d1_ai-agent-saas-security-sandbox_2025-11-18-22-35-26.png)
*pizzacat83氏の講演スライドより*

コンピューティング環境を持つAIエージェントを考えたとき、

ローカルAIエージェントの場合、内部のものを全てアクセスできる可能性がある。
クラウドホスト型AIエージェントの場合、エージェントAでアクセスできるものがエージェントBもアクセスできるなどの可能性がある。

その解決策としてサンドボックス化の必要性が出てくる。

![](../images/codeblue25d1_ai-agent-saas-security-sandbox_2025-11-18-22-40-01.png)
*pizzacat83氏の講演スライドより*

サンドボックスの中に必要な情報のみ保管し、隔離を行うことが求められるが何に基づいて隔離すべきかというと、権限の境界を考えることがヒントとなります。

- 横の権限を考える
  - エージェントAとエージェントBで参照できるものが違う
- 縦の権限を考える
  - テナントAのユーザーは強い権限を持てるがテナントBのユーザーはそれより小さい権限の中でのみ操作させる

などエージェントがどこに対してアクセスできるかの権限の境界を定義することが隔離を考える上で重要となります。

![](../images/codeblue25d1_ai-agent-saas-security-sandbox_2025-11-18-22-41-56.png)
*pizzacat83氏の講演スライドより*

2つのエージェントがPRを作る動作を考えたとき、適切な隔離を行わないとエージェント同士の操作によって衝突が発生することもあります。

適切に隔離を行わないとセキュリティは担保できても適切な動作を行わなくなってしまうことを考慮する必要があります。

![](../images/codeblue25d1_ai-agent-saas-security-sandbox_2025-11-18-22-44-43.png)
*pizzacat83氏の講演スライドより*

AIエージェントをサービスとして提供するにあたって

- VMレベルの隔離を行う高いセキュリティ
- 数秒で起動する高速性
- マルチテナント共有可能な効率性
- エージェントごとの隔離による安定性

といった技術的にはチャレンジング（困難）な課題が必要になる。

![](../images/codeblue25d1_ai-agent-saas-security-sandbox_2025-11-18-22-50-08.png)
*pizzacat83氏の講演スライドより*

Takumiの開発中にこのサンドボックス化の課題について直面し、どのような技術を採用していったかをこの後解説されています。

Takumiは2つの主要なセキュリティ監査のアプローチを持っています。

- ホワイトボックス診断
  - GitHubからリポジトリをクローンしてコードを診断
- ブラックボックス診断
  - 動作しているアプリケーションに対して診断

![](../images/codeblue25d1_ai-agent-saas-security-sandbox_2025-11-18-22-50-47.png)
*pizzacat83氏の講演スライドより*

![](../images/codeblue25d1_ai-agent-saas-security-sandbox_2025-11-18-22-50-56.png)
*pizzacat83氏の講演スライドより*

AIエージェントの隔離を実現するための選択肢として3種類あげています。

- 仮想マシン
  - 高い隔離性を持つが、オーバーヘッドが大きい
- コンテナ
  - 隔離性は低いが、オーバーヘッドが少なく開発環境として普及している
  - Claude Codeは公式の開発コンテナを提供している
- そのほか
  - AnthropicやOpenAI（Codex）、Google（Gemini CLI）など、主要なAIエージェント提供者が実際に使用しているOSレベルの隔離メカニズム

![](../images/codeblue25d1_ai-agent-saas-security-sandbox_2025-11-18-22-58-44.png)
*pizzacat83氏の講演スライドより*

どのようにサンドボックス基盤構造としては画像の通りで、
AIエージェントからリクエストが来たら、APIサーバーよりリクエストを転送しVMノードを立ち上げる構造となっています。

特徴的な面として、

1. 軽量で高速なVMモニタである**Firecracker**を使用して、高い隔離性を実現するVMを多数作成している
2. VMをノード（プール）単位で管理し、APIサーバー経由でエージェントに割り当てている
3. ノードを増やすことで、水平方向（Horizontally）に容易に処理能力を拡張できる設計になっている

![](../images/codeblue25d1_ai-agent-saas-security-sandbox_2025-11-18-23-02-52.png)
*pizzacat83氏の講演スライドより*

Firecrackerとは、AWSによってメンテナンスされているOSSの仮想化技術のことで、Lambdaにも採用されているものです。

https://aws.amazon.com/jp/blogs/news/firecracker-lightweight-virtualization-for-serverless-computing/

パフォーマンスの高さと高いセキュリティを両立した技術であり、最小限の機能のみのためアタックサーフェイスも小さいことが挙げられます。

![](../images/codeblue25d1_ai-agent-saas-security-sandbox_2025-11-18-23-05-40.png)
*pizzacat83氏の講演スライドより*

AIエージェントをサンドボックス化するためのアーキテクチャとして2種類挙げられています。

- 1. Agent in box (エージェントを隔離)
  - エージェント全体（推論と実行環境）が単一のサンドボックスで隔離する
  - Dev ContainerでClaude Codeを動かすイメージ
- 2. Action in box (操作だけを隔離)
  - 推論を行うLLM自体はサンドボックス外にあり、LLMが決定したアクションのみをサンドボックス内で隔離する
  - Devinのようなイメージ

![](../images/codeblue25d1_ai-agent-saas-security-sandbox_2025-11-18-23-11-22.png)
*pizzacat83氏の講演スライドより*

AIエージェントの内部構造としては、以下の図のようなイメージです。

LLM自体がコードを実行しているのではなく、LLMからの提案を受け、エージェントの実装ロジックにより実際のコード実行が行われていることが分かるかと思います。

そのため実行ツール部分は置き換え可能であり、例えば人の承認を挟んだりコンテナを起動したりなどLLMと紐づくものではなく、LLM（推論）と実行環境（アクション）は明確に分離しています。

![](../images/codeblue25d1_ai-agent-saas-security-sandbox_2025-11-18-23-12-33.png)
*pizzacat83氏の講演スライドより*

ではなぜAction in box (操作を隔離)を選ぶのかというと、LLM API Keyに焦点を当てると見えてきます。

Agent in boxの場合、APIキーも一緒に隔離しないといけなくなってしまい、攻撃者がキーにアクセスする機会を得てしまい漏洩のリスクが生じます。
しかしAction in boxの場合、LLM APIキーはサンドボックス環境の外側で管理されており、サンドボックスの実行環境は隔離されていることからLLMへのアクセスもできないので、APIキーは適切に保護された状態となる。サンドボックス環境に入れるべきものを最小限にできるといったメリットがあります。

![](../images/codeblue25d1_ai-agent-saas-security-sandbox_2025-11-18-23-17-11.png)
*pizzacat83氏の講演スライドより*

ただし、内部を隔離することができないクローズドソースのAIエージェントを実行する際には、Action in boxが適している場合もあります。

最後まとめとして、AIエージェントにおける隔離の重要性と隔離のためのアプローチを紹介した旨で締めくくりました。

![](../images/codeblue25d1_ai-agent-saas-security-sandbox_2025-11-18-23-26-59.png)
*pizzacat83氏の講演スライドより*

## まとめ

こちらのセッションでは、
AIエージェントの隔離の重要性とそのアプローチや技術的な側面
について詳しく知ることができました。

この記事がどなたかの役に立つと嬉しいです。